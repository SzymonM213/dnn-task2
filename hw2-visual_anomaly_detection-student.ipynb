{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-shot visual anomaly detection (v2)\n",
    "\n",
    "*This is the second version of the homework with some additional clarifications and changes in the task descriptions (see slack channel for details). The code remains untouched.*\n",
    "\n",
    "In this notebook you'll investigate visual anomaly detection in a typical industrial setting - we don't have much data and we can train only only normal (non-anomalous) examples.\n",
    "Read the [PADIM paper](https://arxiv.org/pdf/2011.08785.pdf) carefully.\n",
    "The code here is based on the original implementation from its authors.\n",
    "\n",
    "If you have any questions - please write them on slack in the channel.\n",
    "\n",
    "### Bibliography\n",
    "\n",
    "1. Defard, T., Setkov, A., Loesch, A., & Audigier, R. (2021). [Padim: a patch distribution modeling framework for anomaly detection and localization](https://arxiv.org/pdf/2011.08785.pdf). In International Conference on Pattern Recognition (pp. 475-489). Cham: Springer International Publishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In case of any problems - please visit [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad/downloads) to get the access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet gdown  # for those who don't run it on Google Colab\n",
    "# !gdown -q '1r7WJeDb-E5zzgQSOx7F7bNWg8kYX3yKE'\n",
    "# !gdown -q '1Kb420ygkN1iBni5Iy_-psLGNoY0gQFk9'\n",
    "# !gdown -q '12wDP9I3aVIr1qLekWY3GLhQO7c6SRhGn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "DATA_PATH = Path('./mvtec_anomaly_detection')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "for class_name in ['bottle', 'metal_nut', 'transistor']:\n",
    "    if not (DATA_PATH / class_name).exists():\n",
    "        with tarfile.open(class_name + '.tar.xz') as tar:\n",
    "            tar.extractall(path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PADIM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from typing import cast, Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends, torch.backends.mps\n",
    "import torch.nn.functional as F\n",
    "from numpy.typing import NDArray\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import morphology\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import wide_resnet50_2, resnet18, Wide_ResNet50_2_Weights, ResNet18_Weights\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "FloatNDArray = NDArray[np.float32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave it as is if you're unsure, this notebook will guess this for you below.\n",
    "DEVICE: Optional[torch.device] = None\n",
    "SEED: int = 42  # do not modify\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 0) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_best_device_for_pytorch() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        device_str = \"cuda\"     # GPU\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device_str = \"mps\"      # Apple silicon\n",
    "    else:\n",
    "        device_str = \"cpu\"      # CPU\n",
    "    return torch.device(device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch with cuda backend.\n",
      "Seeded everything with 42.\n"
     ]
    }
   ],
   "source": [
    "if not DEVICE:\n",
    "    DEVICE = get_best_device_for_pytorch()\n",
    "print(f\"Using PyTorch with {DEVICE} backend.\")\n",
    "\n",
    "seed_all(SEED)\n",
    "print(f\"Seeded everything with {SEED}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVTecDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecDataset(Dataset[Tuple[torch.Tensor, int, torch.Tensor]]):\n",
    "    \"\"\"MVTec dataset of industrial objects with and without anomalies.\n",
    "\n",
    "    Yields (x, y, mask) tuples where:\n",
    "    - x is an RGB image from the class, as float tensor of shape (3, cropsize, cropsize);\n",
    "    - y is an int, 0 for good images, 1 for anomalous images;\n",
    "    - mask is 0 for normal pixels, 1 for anomalous pixels, as float tensor of shape (1, cropsize, cropsize).\n",
    "\n",
    "    Source: https://github.com/xiahaifeng1995/PaDiM-Anomaly-Detection-Localization-master/blob/main/datasets/mvtec.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: Path, class_name: str = 'bottle',\n",
    "                 is_train: bool = True, resize: int = 256, cropsize: int = 224, return_only_indices=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.class_name = class_name\n",
    "        assert (dataset_path / class_name).is_dir(), f'Dataset class not found: {dataset_path / class_name}'\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.resize = resize\n",
    "        self.cropsize = cropsize\n",
    "\n",
    "        # load dataset\n",
    "        self.x, self.y, self.mask = self.load_dataset_folder()\n",
    "\n",
    "        # set transforms\n",
    "        self.transform_x = T.Compose([T.Resize(resize, Image.LANCZOS),\n",
    "                                      T.CenterCrop(cropsize),\n",
    "                                      T.ToTensor(),\n",
    "                                      T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                  std=[0.229, 0.224, 0.225])])\n",
    "        self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n",
    "                                         T.CenterCrop(cropsize),\n",
    "                                         T.ToTensor()])\n",
    "        \n",
    "        self.return_only_indices = return_only_indices\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, torch.Tensor]:\n",
    "        if self.return_only_indices:  # Used for checking the answer of T1.1.\n",
    "            return idx\n",
    "        \n",
    "        x, y, mask = self.x[idx], self.y[idx], self.mask[idx]\n",
    "\n",
    "        x = Image.open(x).convert('RGB')\n",
    "        x = cast(torch.Tensor, self.transform_x(x))\n",
    "\n",
    "        if y == 0:\n",
    "            mask = torch.zeros([1, self.cropsize, self.cropsize])\n",
    "        else:\n",
    "            assert mask is not None\n",
    "            mask = Image.open(mask)\n",
    "            mask = cast(torch.Tensor, self.transform_mask(mask))\n",
    "\n",
    "        return x, y, mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "\n",
    "    def load_dataset_folder(self) -> Tuple[List[Path], List[int], List[Optional[Path]]]:\n",
    "        phase = 'train' if self.is_train else 'test'\n",
    "        x: List[Path] = []\n",
    "        y: List[int] = []\n",
    "        mask: List[Optional[Path]] = []\n",
    "\n",
    "        img_dir = self.dataset_path / self.class_name / phase\n",
    "        gt_dir = self.dataset_path / self.class_name / 'ground_truth'\n",
    "\n",
    "        for img_type_dir in sorted(img_dir.iterdir()):\n",
    "            # Load images.\n",
    "            if not img_type_dir.is_dir():\n",
    "                continue\n",
    "            img_fpath_list = sorted(img_type_dir.glob('*.png'))\n",
    "            x.extend(img_fpath_list)\n",
    "\n",
    "            # Load ground-truth labels and masks.\n",
    "            if img_type_dir.name == 'good':\n",
    "                y.extend([0] * len(img_fpath_list))\n",
    "                mask.extend([None] * len(img_fpath_list))\n",
    "            else:\n",
    "                y.extend([1] * len(img_fpath_list))\n",
    "                mask.extend([gt_dir / img_type_dir.name / (f.stem + '_mask.png')\n",
    "                            for f in img_fpath_list])\n",
    "\n",
    "        assert len(x) == len(y) == len(mask), 'Number of x, y, and mask should be the same.'\n",
    "        return x, y, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_idx(number_of_features: int, max_number_of_features: int) -> torch.Tensor:\n",
    "    assert number_of_features <= max_number_of_features\n",
    "    return torch.tensor(sample(range(0, max_number_of_features), number_of_features))\n",
    "\n",
    "\n",
    "def denormalization(x: FloatNDArray) -> NDArray[np.uint8]:\n",
    "    \"\"\"Denormalize with ImageNet values.\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    return (((x.transpose(1, 2, 0) * std) + mean) * 255.).astype(np.uint8)\n",
    "\n",
    "\n",
    "def embedding_concat(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Concatenate embeddings (along the channel dimension, upscaling y to match x).\n",
    "\n",
    "    Args:\n",
    "        x: Tensor of shape (B, C1, H1, W1).\n",
    "        y: Tensor of shape (B, C2, H2, W2).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape is (B, C1 + C2, H1, W1).\n",
    "    \"\"\"\n",
    "    B, C1, H1, W1 = x.size()\n",
    "    _, C2, H2, W2 = y.size()\n",
    "    s = int(H1 / H2)\n",
    "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
    "    x = x.view(B, C1, s * s, H2, W2)\n",
    "    z = torch.zeros(B, C1 + C2, s * s, H2, W2).to(x.device)\n",
    "    for i in range(s * s):\n",
    "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), dim=1)\n",
    "    z = z.view(B, -1, H2 * W2)\n",
    "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
    "    return z\n",
    "\n",
    "def concatenate_embeddings_from_all_layers(layer_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        embedding_vectors = layer_outputs['layer1']\n",
    "        for layer_name in ['layer2', 'layer3']:\n",
    "            embedding_vectors = embedding_concat(embedding_vectors, layer_outputs[layer_name])\n",
    "        return embedding_vectors\n",
    "\n",
    "def plot_fig(test_img, scores, gts, threshold: float, save_dir: Path, class_name: str):\n",
    "    num = len(scores)\n",
    "    vmax = scores.max() * 255.\n",
    "    vmin = scores.min() * 255.\n",
    "    for i in range(num):\n",
    "        img = test_img[i]\n",
    "        img = denormalization(img)\n",
    "        gt = gts[i].transpose(1, 2, 0).squeeze()\n",
    "        heat_map = scores[i] * 255\n",
    "        mask = scores[i]\n",
    "        mask[mask > threshold] = 1\n",
    "        mask[mask <= threshold] = 0\n",
    "        kernel = morphology.disk(4)\n",
    "        mask = morphology.opening(mask, kernel)\n",
    "        mask *= 255\n",
    "        vis_img = mark_boundaries(img, mask, color=(1, 0, 0), mode='thick')\n",
    "        fig_img, ax_img = plt.subplots(1, 5, figsize=(12, 3))\n",
    "        fig_img.subplots_adjust(right=0.9)\n",
    "        norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        for ax_i in ax_img:\n",
    "            ax_i.axes.xaxis.set_visible(False)\n",
    "            ax_i.axes.yaxis.set_visible(False)\n",
    "        ax_img[0].imshow(img)\n",
    "        ax_img[0].title.set_text('Image')\n",
    "        ax_img[1].imshow(gt, cmap='gray')\n",
    "        ax_img[1].title.set_text('GroundTruth')\n",
    "        ax = ax_img[2].imshow(heat_map, cmap='jet', norm=norm)\n",
    "        ax_img[2].imshow(img, cmap='gray', interpolation='none')\n",
    "        ax_img[2].imshow(heat_map, cmap='jet', alpha=0.5, interpolation='none')\n",
    "        ax_img[2].title.set_text('Predicted heat map')\n",
    "        ax_img[3].imshow(mask, cmap='gray')\n",
    "        ax_img[3].title.set_text('Predicted mask')\n",
    "        ax_img[4].imshow(vis_img)\n",
    "        ax_img[4].title.set_text('Segmentation result')\n",
    "        left = 0.92\n",
    "        bottom = 0.15\n",
    "        width = 0.015\n",
    "        height = 1 - 2 * bottom\n",
    "        rect = [left, bottom, width, height]\n",
    "        cbar_ax = fig_img.add_axes(rect)\n",
    "        cb = plt.colorbar(ax, shrink=0.6, cax=cbar_ax, fraction=0.046)\n",
    "        cb.ax.tick_params(labelsize=8)\n",
    "        font = {\n",
    "            'family': 'serif',\n",
    "            'color': 'black',\n",
    "            'weight': 'normal',\n",
    "            'size': 8,\n",
    "        }\n",
    "        cb.set_label('Anomaly Score', fontdict=font)\n",
    "\n",
    "        fig_img.savefig(save_dir / f'{class_name}_{i}', dpi=100)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(arch: str) -> nn.Module:\n",
    "    if arch == 'resnet18':\n",
    "        model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1, progress=True)\n",
    "        # t_d = 448\n",
    "        # d = 40\n",
    "    elif arch == 'wide_resnet50_2':\n",
    "        model = wide_resnet50_2(weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1, progress=True)\n",
    "        # t_d = 1792\n",
    "        # d = 550\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADIM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import perm\n",
    "\n",
    "\n",
    "class PADIM():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone: str,\n",
    "            device: torch.device,\n",
    "            save_path: Path,\n",
    "            backbone_features_idx: torch.Tensor,\n",
    "            class_names: List[str] = [\"bottle\"],\n",
    "            plot_metrics: bool = False,\n",
    "    ) -> None:\n",
    "        self.arch = backbone\n",
    "        self.device = device\n",
    "        self.model = get_feature_extractor(backbone)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.feature_subset_indices = backbone_features_idx\n",
    "        self.feature_subset_indices.to(device)\n",
    "\n",
    "        self.outputs: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.save_path = save_path\n",
    "        self.plot_metrics = plot_metrics\n",
    "\n",
    "        self.setup_hooks()\n",
    "        (self.save_path / f'temp_{self.arch}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.mean: FloatNDArray  # shape (C, H * W)\n",
    "        self.cov: FloatNDArray  # shape (C, C, H * W)\n",
    "\n",
    "    def setup_hooks(self):\n",
    "        \"\"\"Setup hooks to store model's intermediate outputs.\"\"\"\n",
    "        self.model.layer1[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer1': x}))\n",
    "        self.model.layer2[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer2': x}))\n",
    "        self.model.layer3[-1].register_forward_hook(lambda _, __, x: self.outputs.update({'layer3': x}))\n",
    "\n",
    "    def train_and_test(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> float:\n",
    "        self.train(train_dataloader)\n",
    "        return self.test(test_dataloader)\n",
    "\n",
    "    def train(self, train_dataloader: DataLoader) -> None:\n",
    "        self.train_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
    "        for x, _, _ in tqdm(train_dataloader, desc='Feature extraction (train)'):\n",
    "            # Run model prediction.\n",
    "            with torch.no_grad():\n",
    "                _ = self.model(x.to(DEVICE))\n",
    "            # Get intermediate layer outputs.\n",
    "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3'], list(self.outputs.keys())\n",
    "            for k, v in self.outputs.items():\n",
    "                self.train_outputs[k].append(v.cpu().detach())\n",
    "            # Reset hook outputs.\n",
    "            self.outputs = {}\n",
    "\n",
    "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
    "            {k: torch.cat(v, 0) for k, v in self.train_outputs.items()})\n",
    "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
    "\n",
    "        self.mean, self.cov = self.estimate_multivariate_gaussian(embedding_vectors_subset)\n",
    "        del(self.train_outputs)\n",
    "\n",
    "    def estimate_multivariate_gaussian(self, embedding_vectors: torch.Tensor\n",
    "                                       ) -> Tuple[FloatNDArray, FloatNDArray]:\n",
    "        \"\"\"Calculates multivariate Gaussian distribution.\n",
    "\n",
    "        Takes embeddings of shape (N, C, H, W).\n",
    "        Returns (mean, covariance) of shape (C, H * W) and (C, C, H * W) respectively.\n",
    "        \"\"\"\n",
    "        B, C, H, W = embedding_vectors.size()\n",
    "        embedding_vectors = embedding_vectors.view(B, C, H * W)\n",
    "        mean = torch.mean(embedding_vectors, dim=0).numpy()\n",
    "        cov = torch.zeros(C, C, H * W).numpy()\n",
    "        I = np.identity(C)\n",
    "        for i in tqdm(range(H * W), desc=\"Covariance estimation\"):\n",
    "            cov[:, :, i] = np.cov(embedding_vectors[:, :, i].numpy(), rowvar=False) + 0.01 * I\n",
    "        return mean, cov\n",
    "\n",
    "    def test(self, test_dataloader: DataLoader) -> float:\n",
    "        self.test_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
    "        test_imgs: List[FloatNDArray] = []\n",
    "        gt_list: List[NDArray[Any]] = []\n",
    "        gt_mask_list: List[FloatNDArray] = []\n",
    "\n",
    "        # Extract test set features.\n",
    "        for x, y, mask in tqdm(test_dataloader, desc='Feature extraction (test)', disable=False):\n",
    "            x_shape = x.shape\n",
    "            test_imgs.extend(x.cpu().detach().numpy())\n",
    "            gt_list.extend(y.cpu().detach().numpy())\n",
    "            gt_mask_list.extend(mask.cpu().detach().numpy())\n",
    "            # Run model prediction.\n",
    "            with torch.no_grad():\n",
    "                _ = self.model(x.to(DEVICE))\n",
    "            # Get intermediate layer outputs.\n",
    "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3']\n",
    "            for k, v in self.outputs.items():\n",
    "                self.test_outputs[k].append(v.cpu().detach())\n",
    "            # Reset hook outputs.\n",
    "            self.outputs = {}\n",
    "        gt_mask = np.asarray(gt_mask_list)  # shape (len(test_dataset), 1, H, W)\n",
    "\n",
    "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
    "            {k: torch.cat(v, 0) for k, v in self.test_outputs.items()})\n",
    "        # shape (len(test_dataset), len(feature_subset_indices), H1, W1)\n",
    "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
    "\n",
    "        distances = self.calculate_distances(embedding_vectors_subset)\n",
    "        \n",
    "        score_map = self.prepare_anomaly_map((x_shape[2], x_shape[3]), distances)\n",
    "\n",
    "        img_fpr, img_tpr, img_auroc = self.calculate_auroc_image_level(score_map, gt_list)\n",
    "        pxl_fpr, pxl_tpr, pxl_auroc = self.calculate_auroc_pixel_level(score_map, gt_mask)\n",
    "\n",
    "        if self.plot_metrics:\n",
    "            print(f'[TEST] Image AUROC: {img_auroc:.3f}')\n",
    "            print(f'[TEST] Pixel AUROC: {pxl_auroc:.3f}')\n",
    "            threshold = self.calculate_optimal_threshold(score_map, gt_mask)\n",
    "            self.plot_test_results_for_class(gt_mask_list, test_imgs, score_map, threshold, img_fpr, img_tpr, img_auroc, pxl_fpr, pxl_tpr, pxl_auroc)\n",
    "\n",
    "        return pxl_auroc\n",
    "\n",
    "    # TODO: Some of your code for Task 1 goes here. You can add more functions if needed, but use the ones below - we will use them for checking your solution.\n",
    "    def test_permutation_importance(self, val_dataloader: DataLoader, features_to_permute: List[int]) -> List[float]:\n",
    "        \"\"\"Runs a series of tests on `val_dataloader`.\n",
    "        Returns a list of pixelwise AUROCs, where the n-th element of the list is generated by testing the embeddings from `permute_feature(embeddings, features_to_permute[n]).\"\"\"\n",
    "        self.test_outputs: Dict[str, List[torch.Tensor]] = {'layer1': [], 'layer2': [], 'layer3': []}\n",
    "        gt_mask_list: List[FloatNDArray] = []\n",
    "\n",
    "        # Extract test set features.\n",
    "        for x, _, mask in val_dataloader:\n",
    "            x_shape = x.shape\n",
    "            gt_mask_list.extend(mask.cpu().detach().numpy())\n",
    "            # Run model prediction.\n",
    "            with torch.no_grad():\n",
    "                _ = self.model(x.to(DEVICE))\n",
    "            # Get intermediate layer outputs.\n",
    "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3']\n",
    "            for k, v in self.outputs.items():\n",
    "                self.test_outputs[k].append(v.cpu().detach())\n",
    "            # Reset hook outputs.\n",
    "            self.outputs = {}\n",
    "        gt_mask = np.asarray(gt_mask_list)  # shape (len(test_dataset), 1, H, W)\n",
    "\n",
    "        embedding_vectors = concatenate_embeddings_from_all_layers(\n",
    "            {k: torch.cat(v, 0) for k, v in self.test_outputs.items()})\n",
    "\n",
    "        s = []\n",
    "\n",
    "        embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices.cpu())\n",
    "        # for feature_to_permute in features_to_permute:\n",
    "        for feature_to_permute in tqdm(features_to_permute, desc='Feature permutation', disable=False):\n",
    "            shuffled_embedding_vectors_subset = self.permute_feature(embedding_vectors_subset, feature_to_permute)\n",
    "            distances = self.calculate_distances(shuffled_embedding_vectors_subset)\n",
    "            score_map = self.prepare_anomaly_map((x_shape[2], x_shape[3]), distances)\n",
    "\n",
    "            # pxl_fpr, pxl_tpr, pxl_auroc = self.calculate_auroc_pixel_level(score_map, gt_mask)\n",
    "            pxl_auroc = float(roc_auc_score(gt_mask.flatten(), score_map.flatten()))\n",
    "\n",
    "            s.append(pxl_auroc)\n",
    "            \n",
    "        return s\n",
    "\n",
    "    def permute_feature(self, embedding_vectors_subset: torch.Tensor, number_of_feature_to_permute: int) -> torch.Tensor:\n",
    "        \"\"\"Permutes the embeddings.\n",
    "\n",
    "        Takes embeddings of shape (N, C, H, W) and feature number to permute.\n",
    "        Returns embeddings with the same shape. See the description of T1 for the details. \n",
    "        \"\"\"\n",
    "        B, _, H, W = embedding_vectors_subset.shape\n",
    "        new_embedding_vectors_subset = torch.clone(embedding_vectors_subset)\n",
    "        for b in range(B):\n",
    "            perm_indices = torch.randperm(H * W)\n",
    "            new_embedding_vectors_subset[b, number_of_feature_to_permute] = embedding_vectors_subset[b, number_of_feature_to_permute].flatten()[perm_indices].view(H, W)\n",
    "        return new_embedding_vectors_subset\n",
    "    # TODO: End of your code for Task 1 (here)\n",
    "\n",
    "    def plot_test_results_for_class(self, gt_mask_list, test_imgs,\n",
    "                                    score_map, threshold: float,\n",
    "                                    img_fpr, img_tpr, img_auroc: float,\n",
    "                                    pxl_fpr, pxl_tpr, pxl_auroc: float):\n",
    "        _, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        ax[0].plot(img_fpr, img_tpr, label=f'Image AUROC: {img_auroc:.3f}')\n",
    "        ax[1].plot(pxl_fpr, pxl_tpr, label=f'Pixel AUROC: {pxl_auroc:.3f}')\n",
    "\n",
    "        save_dir = self.save_path / f'pictures_{self.arch}'\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plot_fig(test_imgs, score_map, gt_mask_list,\n",
    "                 threshold, save_dir, \"\")\n",
    "\n",
    "    def calculate_auroc_image_level(self, score_map: FloatNDArray, gt_list: List[NDArray[Any]]) -> Tuple[FloatNDArray, FloatNDArray, float]:\n",
    "        \"\"\"Calculate image-level AUROC score.\"\"\"\n",
    "        img_scores = score_map.reshape(score_map.shape[0], -1).max(axis=1)\n",
    "        fpr, tpr, _ = roc_curve(gt_list, img_scores)  # false-positive-rates and true-positive-rates for consecutive thresholds (for plotting).\n",
    "        img_auroc = roc_auc_score(gt_list, img_scores)\n",
    "        return fpr, tpr, float(img_auroc)\n",
    "\n",
    "    def calculate_auroc_pixel_level(self, score_map: FloatNDArray, gt_mask: FloatNDArray) -> Tuple[FloatNDArray, FloatNDArray, float]:\n",
    "        \"\"\"Calculate per-pixel level AUROC.\"\"\"\n",
    "        assert score_map.shape == gt_mask.squeeze().shape, f\"{score_map.shape=}, {gt_mask.shape=}\"\n",
    "        fpr, tpr, _ = roc_curve(gt_mask.flatten(), score_map.flatten())\n",
    "        per_pixel_auroc = roc_auc_score(gt_mask.flatten(), score_map.flatten())\n",
    "        return fpr, tpr, float(per_pixel_auroc)\n",
    "\n",
    "    def calculate_optimal_threshold(self, score_map: FloatNDArray, gt_mask: FloatNDArray) -> float:\n",
    "        \"\"\"Calculate the optimal threshold with regard to F1 score.\"\"\"\n",
    "        assert score_map.shape == gt_mask.squeeze().shape\n",
    "        precision, recall, thresholds = precision_recall_curve(\n",
    "            gt_mask.flatten(), score_map.flatten())\n",
    "        a = 2 * precision * recall\n",
    "        b = precision + recall\n",
    "        f1 = np.divide(a, b, out=np.zeros_like(a), where=(b != 0))\n",
    "        threshold = thresholds[np.argmax(f1)]\n",
    "        return threshold\n",
    "\n",
    "    def calculate_distances(self, embedding_vectors: torch.Tensor) -> FloatNDArray:\n",
    "        \"\"\"Calculate Mahalanobis distance of each embedding vector from self.mean.\n",
    "\n",
    "        For embeddings of shape (N, C, H, W), returns shape (N, H, W).\n",
    "        \"\"\"\n",
    "        B, C, H, W = embedding_vectors.size()\n",
    "        embedding_vectors = embedding_vectors.view(B, C, H * W).numpy()\n",
    "        dist_list: List[List[np.float64]] = []\n",
    "        for i in range(H * W):\n",
    "            mean = self.mean[:, i]\n",
    "            conv_inv = np.linalg.inv(self.cov[:, :, i])\n",
    "            dist = [mahalanobis(sample[:, i], mean, conv_inv)\n",
    "                    for sample in embedding_vectors]\n",
    "            dist_list.append(dist)\n",
    "\n",
    "        return np.array(dist_list).transpose(1, 0).reshape(B, H, W)\n",
    "\n",
    "    def prepare_anomaly_map(self, shape: Tuple[int, int], distances: FloatNDArray) -> FloatNDArray:\n",
    "        \"\"\"Upsample distances to `shape`, apply Gaussian smoothing, and normalize to [0,1].\n",
    "\n",
    "        For distances of shape (N, H, W) and `shape` equal to (H2, W2), returns shape (N, H2, W2).\n",
    "        \"\"\"\n",
    "        dists = torch.Tensor(distances).unsqueeze(1)\n",
    "        shape = (dists.shape[0],) + shape\n",
    "        score_map = cast(FloatNDArray, F.interpolate(\n",
    "            dists, size=shape[2], mode='bilinear', align_corners=False).squeeze().numpy())\n",
    "        for i in range(score_map.shape[0]):\n",
    "            score_map[i] = gaussian_filter(score_map[i], sigma=4)\n",
    "\n",
    "        min_score, max_score = score_map.min(), score_map.max()\n",
    "        return (score_map - min_score) / (max_score - min_score + 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see whether it works.\n",
    "Take a look to the `SAVE_PATH` to inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== bottle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 105/105 [00:08<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([209, 448, 56, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Covariance estimation:   0%|          | 0/3136 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "seed_all(SEED)\n",
    "CLASS_NAMES = [\n",
    "            'bottle', #'metal_nut'\n",
    "            # 'cable', 'capsule', 'carpet', 'grid', 'hazelnut', 'leather',\n",
    "            # 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper'\n",
    "        ]\n",
    "BATCH_SIZE = 2\n",
    "RESIZE = 256 * 1\n",
    "CROP_SIZE = 224 * 1\n",
    "BACKBONE = \"resnet18\"\n",
    "NUMBER_OF_BACKBONE_FEATURES = 50\n",
    "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
    "\n",
    "run_timestamp = time.time()\n",
    "for class_name in CLASS_NAMES:\n",
    "    print('=' * 10, class_name)\n",
    "    SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
    "\n",
    "    train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "    test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "    padim = PADIM(\n",
    "        backbone=BACKBONE,\n",
    "        device=DEVICE,\n",
    "        backbone_features_idx=sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES),\n",
    "        save_path=SAVE_PATH,\n",
    "        plot_metrics=True,\n",
    "    )\n",
    "\n",
    "    padim.train_and_test(\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Finding the right features (40%)\n",
    "\n",
    "The authors of the paper argue that it doesn't really matter how we choose a subset of features. Let's make some steps towards exploring whether it's true for three different classes (`bottle`, `transistor`, `metal_nut`).\n",
    "Design an experiment which will rank the ResNet18 features by its importance. To do so, we'll implement our variation of [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html#outline-of-the-permutation-importance-algorithm) on a subset of features produced by the backbone. \n",
    "\n",
    "## 1.1 Preparing the data\n",
    "- Using the test dataset, create `val_dataloader` (every even sample from the original test dataset) and `test_dataloader` (every odd sample). `SubsetRandomSampler` might be handy here.\n",
    "- Then, create 3-fold cross validation-like process in which you'll train three PADIM models on the first 100 ResNet features in three equally sized subsets of train dataset in which you discard 1/3 of the data ($\\texttt{padim}_{k}\\texttt{.train}(\\texttt{train\\_dataloader}_k)$) (see below). Again, `SubsetRandomSampler` might be handy here.\n",
    "\n",
    "In other words, you should have:\n",
    "\n",
    "- for $k=0$, the first 10 images indexes from the train dataset we should train on are `[1, 2, 4, 5, 7, 8, 10, 11, 13, 14]`,\n",
    "- for $k=1$, that's `[0, 2, 3, 5, 6, 8, 9, 11, 12, 14]`,\n",
    "- and for $k=2$, that's `[0, 1, 3, 4, 6, 7, 9, 10, 12, 13]`.\n",
    "\n",
    "For val and train, you should have `[0, 2, 4, ...]` and `[1, 3, 5, ...]` respectively (from the test dataset).\n",
    "\n",
    "Don't worry about the sampling order.\n",
    "Use these names for DataLoaders `val_dataloader`, `test_dataloader`. For k-fold training, store dataloaders in `train_dataloaders: List[DataLoader]`, where each element represent different $k$. \n",
    "For each class, store the results in `dataloaders` dictionary (the variable is defined in the code below) - we will use this to check your solution.\n",
    "\n",
    "## 1.2 Calculating the importances\n",
    "- In a given fold, each $j$-th feature shall be ranked based on the pixel-wise AUROC difference between the output of that model ($s_{k} \\leftarrow \\texttt{padim}_{k}\\texttt{.test}(\\texttt{val\\_dataloader})$) and the output with the model with permuted $j$-th feature ($s_{k, j} \\leftarrow \\texttt{padim}_{k}\\texttt{.test\\_permutation\\_importance}(\\texttt{val\\_dataloader, features\\_to\\_permute=}[j])$). In practice you can pass all the numbers of features to permute (instead of 1-element list and do the loop inside the method. See also `test_permutation_importance` method stub above.\n",
    "- Implement `permute_feature` method as follows: given the tensor with embeddings with shape `[B, C, H, W]`, by permutation of the $j$-th feature we mean randomly swapped values for $C=j$. Although (ideally) the order of swapping shall be **different** for every image, we don't require you to strictly guarantee that you won't get the same permutation twice (what matters here is not using the same permutation for **every** sample - you can e.g. use distinct calls to a shuffling function for every sample). In other words, for every image $b$ and feature $j$ you need to shuffle the last two dimensions (marked as stars in `[b, j, *, *]`) in an (ideally) unique manner.\n",
    "- Then, calculate the mean importance $i$ averaged on these folds and plot weights importance for the class ($i_j \\leftarrow \\frac{1}{K} \\sum_{k} ( s_k -  s_{k, j} )$, where $K$ is the number of folds).\n",
    "- Append results in `results` dictionary, where keys are class names and values are the lists of averaged feature importances (from feature 0 to feature 99).\n",
    "\n",
    "## 1.3 Drawing conclusions\n",
    "\n",
    "- Finally, for every class train three models on the full training data and evaluate it on the `test_dataloader`. The first model shall use the first 10 features, the second shall use worst 10 features (in terms of feature importance), and the third shall contain the best 10 features.\n",
    "- Write your conclusions (with the things enlisted below in mind). Simply plotting charts or outputting logs without any comment doesn't qualify as an answer to a question.\n",
    "\n",
    "Note 1: Limit yourself to the first 100 features of ResNet18. If you want, you can go with all of available features instead of 100, but it'll take some time to calculate. Converting parts of the code to PyTorch and running on GPU might change a lot here, but this is not evaluated in this exercise. This experiment can be calculated without GPU in less than one hour anyway.\n",
    "\n",
    "Note 2: If you'd like to be fully covered, one needs to explore if the features are correlated, as this might bias the results of feature importance calculations. However, this is not evaluated in this task for the sake of simplicity (that is, examining the 100 first features without worrying about correlated features are enough to get 100% from this task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "# CLASS_NAMES = ['bottle', 'transistor', 'metal_nut']\n",
    "CLASS_NAMES = ['bottle']\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "RESIZE = 256 * 2 // 4\n",
    "CROP_SIZE = 224 * 2 // 4\n",
    "BACKBONE = \"resnet18\"\n",
    "NUMBER_OF_BACKBONE_FEATURES = 10\n",
    "MAX_NUMBER_OF_BACKBONE_FEATURES = 100  # 448\n",
    "folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702836174.4891906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 139/139 [00:05<00:00, 24.15it/s]\n",
      "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 4732.12it/s]\n",
      "Feature extraction (train): 100%|██████████| 139/139 [00:04<00:00, 27.89it/s]\n",
      "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 4697.77it/s]\n",
      "Feature extraction (train): 100%|██████████| 140/140 [00:05<00:00, 27.85it/s]\n",
      "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 5222.49it/s]\n"
     ]
    }
   ],
   "source": [
    "seed_all(SEED)\n",
    "results = {c: [0] * MAX_NUMBER_OF_BACKBONE_FEATURES for c in CLASS_NAMES}\n",
    "\n",
    "run_timestamp = time.time()\n",
    "print(f\"{run_timestamp}\")\n",
    "\n",
    "idx_all_fatures = torch.Tensor(range(MAX_NUMBER_OF_BACKBONE_FEATURES)).int()\n",
    "idx_first_n_features = torch.Tensor(range(NUMBER_OF_BACKBONE_FEATURES)).int()\n",
    "\n",
    "dataloaders = {c: {\"val_dataloader\": None, \"test_dataloader\": None, \"train_dataloaders\": None} for c in CLASS_NAMES}\n",
    "\n",
    "# TODO: Your code for T1.1, T1.2, and T1.3 goes below. Don't forget to write `test_permutation_importance` and `permute_feature` above in the PADIM code.\n",
    "# T1.1\n",
    "\n",
    "models = {c: [] for c in CLASS_NAMES}\n",
    "for c in CLASS_NAMES:\n",
    "\n",
    "    SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
    "\n",
    "    train_dataset = MVTecDataset(DATA_PATH, class_name=c, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "    test_dataset = MVTecDataset(DATA_PATH, class_name=c, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "\n",
    "    val_indices = list(range(0, len(test_dataset), 2))\n",
    "    test_indices = list(range(1, len(test_dataset), 2))\n",
    "\n",
    "\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    dataloaders[c][\"val_dataloader\"] = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, pin_memory=True)\n",
    "    dataloaders[c][\"test_dataloader\"] = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, pin_memory=True)\n",
    "    \n",
    "    dataloaders[c][\"train_dataloaders\"] = []\n",
    "    for k in range(folds):\n",
    "        train_indices = [i for i in range(len(train_dataset)) if i % folds != k]\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        dataloaders[c][\"train_dataloaders\"].append(DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, pin_memory=True))\n",
    "        padim = PADIM(\n",
    "            backbone=BACKBONE,\n",
    "            device=DEVICE,\n",
    "            backbone_features_idx=idx_all_fatures,\n",
    "            save_path=SAVE_PATH,\n",
    "        )\n",
    "        padim.train(dataloaders[c][\"train_dataloaders\"][k])\n",
    "        models[c].append(padim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (test):   0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (test): 100%|██████████| 42/42 [00:02<00:00, 20.80it/s]\n",
      "Feature permutation: 100%|██████████| 100/100 [03:40<00:00,  2.20s/it]\n",
      "Feature extraction (test): 100%|██████████| 42/42 [00:01<00:00, 21.25it/s]\n",
      "Feature permutation: 100%|██████████| 100/100 [06:15<00:00,  3.75s/it]\n",
      "Feature extraction (test): 100%|██████████| 42/42 [00:01<00:00, 22.29it/s]\n",
      "Feature permutation: 100%|██████████| 100/100 [06:27<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# T1.2\n",
    "for c in CLASS_NAMES:\n",
    "    i = [0] * MAX_NUMBER_OF_BACKBONE_FEATURES\n",
    "    for k in range(folds):\n",
    "        s = models[c][k].test(dataloaders[c][\"val_dataloader\"])\n",
    "        s_j = models[c][k].test_permutation_importance(dataloaders[c][\"val_dataloader\"], list(range(MAX_NUMBER_OF_BACKBONE_FEATURES)))\n",
    "        # tmp = [s - s_j[j] for j in range(MAX_NUMBER_OF_BACKBONE_FEATURES)]\n",
    "        # i = [i[j] + tmp[j] for j in range(MAX_NUMBER_OF_BACKBONE_FEATURES)]\n",
    "        for j in range(MAX_NUMBER_OF_BACKBONE_FEATURES):\n",
    "            results[c][j] += (s - s_j[j]) / folds\n",
    "    # i = [i[j] / folds for j in range(MAX_NUMBER_OF_BACKBONE_FEATURES)]\n",
    "    # results[c] = i\n",
    "\n",
    "# print(i)\n",
    "\n",
    "\n",
    "# print(models['bottle'][0].test(dataloaders['bottle'][\"val_dataloader\"]))\n",
    "# print(models['bottle'][0].test_permutation_importance(dataloaders['bottle'][\"val_dataloader\"], [0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62   -0.000215\n",
      "67   -0.000035\n",
      "91   -0.000015\n",
      "2    -0.000005\n",
      "99   -0.000003\n",
      "84    0.000005\n",
      "73    0.000015\n",
      "35    0.000044\n",
      "65    0.000063\n",
      "76    0.000064\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "s = pd.Series(results['bottle'])\n",
    "print(s.sort_values(ascending=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 209/209 [00:08<00:00, 24.80it/s]\n",
      "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 13311.69it/s]\n",
      "Feature extraction (test): 100%|██████████| 41/41 [00:01<00:00, 22.89it/s]\n",
      "Feature extraction (train): 100%|██████████| 209/209 [00:08<00:00, 24.16it/s]\n",
      "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 14576.73it/s]\n",
      "Feature extraction (test): 100%|██████████| 41/41 [00:01<00:00, 23.38it/s]\n",
      "Feature extraction (train): 100%|██████████| 209/209 [00:07<00:00, 26.59it/s]\n",
      "Covariance estimation: 100%|██████████| 784/784 [00:00<00:00, 14044.97it/s]\n",
      "Feature extraction (test): 100%|██████████| 41/41 [00:01<00:00, 25.72it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# T1.3\n",
    "full_train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "labels = [\"first 10 features\", \"worst 10 features\", \"best 10 features\"]\n",
    "\n",
    "for c in CLASS_NAMES:\n",
    "    s = pd.Series(results[c])\n",
    "    padims = []\n",
    "    aurocs = []\n",
    "    padims.append(PADIM(\n",
    "        backbone=BACKBONE,\n",
    "        device=DEVICE,\n",
    "        backbone_features_idx=idx_first_n_features,\n",
    "        save_path=SAVE_PATH,\n",
    "    ))\n",
    "    padims.append(PADIM(\n",
    "        backbone=BACKBONE,\n",
    "        device=DEVICE,\n",
    "        backbone_features_idx=torch.tensor(s.sort_values(ascending=True).index[:NUMBER_OF_BACKBONE_FEATURES]).int(),\n",
    "        save_path=SAVE_PATH,\n",
    "    ))\n",
    "    padims.append(PADIM(\n",
    "        backbone=BACKBONE,\n",
    "        device=DEVICE,\n",
    "        backbone_features_idx=torch.tensor(s.sort_values(ascending=False).index[:NUMBER_OF_BACKBONE_FEATURES]).int(),\n",
    "        save_path=SAVE_PATH,\n",
    "    ))\n",
    "    for i, padim in enumerate(padims):\n",
    "        aurocs.append(padim.train_and_test(\n",
    "            train_dataloader=full_train_dataloader,\n",
    "            test_dataloader=dataloaders[c][\"test_dataloader\"],\n",
    "        ))\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.bar(labels, aurocs, label=c, color=['skyblue', 'salmon', 'lightgreen'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 44, 51, 60, 87, 10, 21, 43, 57, 14]\n",
      "[62, 73, 67, 91, 99, 96, 84, 66, 2, 35]\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82]\n",
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81]\n",
      "[1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 58, 59, 61, 62, 64, 65, 67, 68, 70, 71, 73, 74, 76, 77, 79, 80, 82, 83, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 100, 101, 103, 104, 106, 107, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 124, 125, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 142, 143, 145, 146, 148, 149, 151, 152, 154, 155, 157, 158, 160, 161, 163, 164, 166, 167, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 187, 188, 190, 191, 193, 194, 196, 197, 199, 200, 202, 203, 205, 206, 208]\n",
      "[0, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 35, 36, 38, 39, 41, 42, 44, 45, 47, 48, 50, 51, 53, 54, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 71, 72, 74, 75, 77, 78, 80, 81, 83, 84, 86, 87, 89, 90, 92, 93, 95, 96, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 116, 117, 119, 120, 122, 123, 125, 126, 128, 129, 131, 132, 134, 135, 137, 138, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 158, 159, 161, 162, 164, 165, 167, 168, 170, 171, 173, 174, 176, 177, 179, 180, 182, 183, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 200, 201, 203, 204, 206, 207]\n",
      "[0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 61, 63, 64, 66, 67, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 87, 88, 90, 91, 93, 94, 96, 97, 99, 100, 102, 103, 105, 106, 108, 109, 111, 112, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 138, 139, 141, 142, 144, 145, 147, 148, 150, 151, 153, 154, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 171, 172, 174, 175, 177, 178, 180, 181, 183, 184, 186, 187, 189, 190, 192, 193, 195, 196, 198, 199, 201, 202, 204, 205, 207, 208]\n"
     ]
    }
   ],
   "source": [
    "# Run at the end, but do not modify - we will use this to asses your output.\n",
    "for c in CLASS_NAMES:\n",
    "    s = pd.Series(results[c])\n",
    "    print(s.sort_values(ascending=False)[:10].index.tolist())\n",
    "    print(s.sort_values(ascending=True)[:10].index.tolist())\n",
    "\n",
    "def get_sorted_indices(loader):\n",
    "    loader.dataset.return_only_indices = True\n",
    "    indices = sorted([x.item() for x in loader])\n",
    "    loader.dataset.return_only_indices = False\n",
    "    return indices\n",
    "\n",
    "for c in CLASS_NAMES:\n",
    "    print(get_sorted_indices(dataloaders[c][\"val_dataloader\"]))\n",
    "    print(get_sorted_indices(dataloaders[c][\"test_dataloader\"]))\n",
    "    for v in dataloaders[c][\"train_dataloaders\"]:\n",
    "        print(get_sorted_indices(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Improving PADIM with Online Covariance Estimation\n",
    "\n",
    "This implementation of PADIM can be improved in numerous ways. In this exercise, you'll try to indicate its shortcomings and provide some means to mitigate them.\n",
    "\n",
    "#### 2.1. PADIM's training complexity (15%)\n",
    "\n",
    "- Identify the key operations contributing to the algorithm's training space complexity *in this implementation*. Don't focus on the backbone, as it is not the part of the algorithm (however, its output is).\n",
    "- Shortly discuss the implications for scalability. You can support your claims by charts if needed.\n",
    "\n",
    "*Hint: this doesn't need to be super formal analysis - it's about fiding the \"worst\" parts of this implementation. You can support your claims with a chart and brief description (e.g. \"X dominates the complexity, as it's quadratic.\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Your answer to task 2.1 goes here```\\\n",
    "The most expensive operation in this algorithm is appending the train_outputs lists by the outputs for every image. ```train_outputs``` (and as a consequence, ```embedding_vectors```) size is batch size (number of training images) * number of features * sum of layer output sizes. This size is even bigger for ```embedding_vectors``` tensor because we upscale the second and third layer output to be equal H * W which is size of the first layer output. This tensor dominates the whole algorithm memory complexity.\n",
    "\n",
    "As a result, if we think about scaling up the training size, we see that we need to store all the outputs for every image to compute covariance matrix later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here (if needed)\n",
    "\n",
    "padim = PADIM(\n",
    "    backbone=BACKBONE,\n",
    "    device=DEVICE,\n",
    "    backbone_features_idx=idx_all_fatures,\n",
    "    save_path=SAVE_PATH,\n",
    ")\n",
    "\n",
    "%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Online mean and covariance (35%)\n",
    "Implement a PyTorch version of [online covariance matrix estimation](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online) in the training as an alternative to the current method in PADIM.\n",
    "Calculate the mean in an online fashion as well.\n",
    "Your implementation shall run on the selected `torch.device` (such as GPU). \n",
    "No need to reimplement the testing routine to online in this exercise (although it'd be nice to have for Task 1), albeit small changes might be necessary (such as conversion from `torch.Tensor` to `np.ndarray`).\n",
    "\n",
    "Passing criteria:\n",
    "```python\n",
    "torch.allclose(padim_online.mean, torch.Tensor(padim_offline.mean).to(DEVICE), atol=0.01)\n",
    "torch.allclose(padim_online.cov, torch.Tensor(padim_offline.cov).to(DEVICE), atol=0.01)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PADIMWithOnlineCovariance(PADIM):\n",
    "\n",
    "    ### TODO: Your code goes here\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone: str,\n",
    "            device: torch.device,\n",
    "            save_path: Path,\n",
    "            backbone_features_idx: List[int],\n",
    "            class_names=...,\n",
    "            plot_metrics=False,\n",
    "            ) -> None:\n",
    "        super().__init__(backbone, device, save_path, backbone_features_idx, class_names, plot_metrics)\n",
    "\n",
    "    def train(self, train_dataloader: DataLoader, C: int, H: int, W: int):\n",
    "        \"\"\"C, H, W come from the size of embeddings: [B, C, H, W]\"\"\"\n",
    "        self.mean = torch.zeros(C, H * W).to(DEVICE)\n",
    "        self.cov = torch.zeros(C, C, H * W).to(DEVICE)\n",
    "        N = 0\n",
    "        for xs, _, _ in tqdm(train_dataloader, desc='Feature extraction (train)'):\n",
    "            # Run model prediction.\n",
    "            with torch.no_grad():\n",
    "                _ = self.model(xs.to(DEVICE))\n",
    "            # Get intermediate layer outputs.\n",
    "            assert list(self.outputs.keys())  == ['layer1', 'layer2', 'layer3'], list(self.outputs.keys())\n",
    "            embedding_vectors = concatenate_embeddings_from_all_layers(self.outputs).to(DEVICE)\n",
    "            embedding_vectors_subset = torch.index_select(embedding_vectors, 1, self.feature_subset_indices).to(DEVICE)\n",
    "            for x in embedding_vectors_subset:\n",
    "                N += 1\n",
    "                x = x.view(C, H * W)\n",
    "                \n",
    "                delta = x - self.mean\n",
    "                self.mean += delta / N\n",
    "                delta2 = x - self.mean\n",
    "\n",
    "                self.cov += torch.einsum('ji,ki->jki', delta, delta2)\n",
    "            \n",
    "            # Reset hook outputs.\n",
    "            self.outputs = {}\n",
    "        self.cov /= N - 1\n",
    "        self.cov += torch.eye(C).unsqueeze(-1).to(DEVICE) * 0.01\n",
    "    ### END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train):   0%|          | 0/209 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m padim_offline \u001b[38;5;241m=\u001b[39m PADIM(\n\u001b[1;32m     24\u001b[0m     backbone\u001b[38;5;241m=\u001b[39mBACKBONE,\n\u001b[1;32m     25\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     plot_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[43mpadim_offline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m padim_online \u001b[38;5;241m=\u001b[39m PADIMWithOnlineCovariance(\n\u001b[1;32m     33\u001b[0m     backbone\u001b[38;5;241m=\u001b[39mBACKBONE,\n\u001b[1;32m     34\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     plot_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m padim_online\u001b[38;5;241m.\u001b[39mtrain(train_dataloader, NUMBER_OF_BACKBONE_FEATURES, \u001b[38;5;28mint\u001b[39m(CROP_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mint\u001b[39m(CROP_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[10], line 51\u001b[0m, in \u001b[0;36mPADIM.train\u001b[0;34m(self, train_dataloader)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, _, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature extraction (train)\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Run model prediction.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 51\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Get intermediate layer outputs.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mkeys())  \u001b[38;5;241m==\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer3\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do not modify\n",
    "seed_all(SEED)\n",
    "class_name = 'bottle'\n",
    "BATCH_SIZE = 1\n",
    "RESIZE = 256 * 1\n",
    "CROP_SIZE = 224 * 1\n",
    "BACKBONE = \"resnet18\"\n",
    "NUMBER_OF_BACKBONE_FEATURES = 30\n",
    "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
    "# DEVICE=\"cpu\"\n",
    "\n",
    "indices = sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES).to(DEVICE)\n",
    "\n",
    "run_timestamp = time.time()\n",
    "SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
    "\n",
    "train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "test_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=False, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "\n",
    "padim_offline = PADIM(\n",
    "    backbone=BACKBONE,\n",
    "    device=DEVICE,\n",
    "    backbone_features_idx=indices,\n",
    "    save_path=SAVE_PATH,\n",
    "    plot_metrics=True,\n",
    ")\n",
    "padim_offline.train(train_dataloader)\n",
    "\n",
    "padim_online = PADIMWithOnlineCovariance(\n",
    "    backbone=BACKBONE,\n",
    "    device=DEVICE,\n",
    "    backbone_features_idx=indices,\n",
    "    save_path=SAVE_PATH,\n",
    "    plot_metrics=True,\n",
    ")\n",
    "padim_online.train(train_dataloader, NUMBER_OF_BACKBONE_FEATURES, int(CROP_SIZE/4), int(CROP_SIZE/4))\n",
    "\n",
    "torch.allclose(padim_online.mean, torch.Tensor(padim_offline.mean).to(DEVICE), atol=0.01) and torch.allclose(padim_online.cov, torch.Tensor(padim_offline.cov).to(DEVICE), atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5762786865234375e-07\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(torch.abs(padim_online.cov - torch.Tensor(padim_offline.cov).to(DEVICE))).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Performance experiments (10%)\n",
    "If you completed task 2.2, design experiments to empirically compare space and memory performance of PADIM training with both traditional and online covariance matrix estimation. Write short conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 209/209 [00:09<00:00, 21.46it/s]\n",
      "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 3507.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 8958.67 MiB, increment: 3804.39 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 209/209 [00:17<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5800.46 MiB, increment: 239.60 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 104/104 [00:05<00:00, 19.20it/s]\n",
      "Covariance estimation: 100%|██████████| 3136/3136 [00:00<00:00, 4103.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 7137.47 MiB, increment: 1576.01 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 104/104 [00:09<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5746.09 MiB, increment: 239.28 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train): 100%|██████████| 418/418 [00:18<00:00, 22.15it/s]\n",
      "Covariance estimation: 100%|██████████| 3136/3136 [00:01<00:00, 1736.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 11579.20 MiB, increment: 6072.50 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction (train):  96%|█████████▋| 403/418 [00:31<00:01, 11.82it/s]"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "from memory_profiler import profile, memory_usage\n",
    "\n",
    "seed_all(SEED)\n",
    "class_name = 'bottle'\n",
    "BATCH_SIZE = 1\n",
    "RESIZE = 256 * 1\n",
    "CROP_SIZE = 224 * 1\n",
    "BACKBONE = \"resnet18\"\n",
    "NUMBER_OF_BACKBONE_FEATURES = 100\n",
    "MAX_NUMBER_OF_BACKBONE_FEATURES = 448\n",
    "DEVICE = torch.device('cpu') # to use the same type of memory for both models\n",
    "\n",
    "indices = sample_idx(NUMBER_OF_BACKBONE_FEATURES, MAX_NUMBER_OF_BACKBONE_FEATURES).to(DEVICE)\n",
    "\n",
    "run_timestamp = time.time()\n",
    "SAVE_PATH = Path(f\"./results/{run_timestamp}/{class_name}\")\n",
    "\n",
    "train_dataset = MVTecDataset(DATA_PATH, class_name=class_name, is_train=True, resize=RESIZE, cropsize=CROP_SIZE)\n",
    "big_train_dataset = torch.utils.data.ConcatDataset([train_dataset] * 2)\n",
    "\n",
    "train_dataloaders = []\n",
    "train_dataloaders.append(DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)) # normal size\n",
    "train_dataloaders.append(DataLoader(big_train_dataset, batch_size=BATCH_SIZE, pin_memory=True, \n",
    "                                    sampler=SubsetRandomSampler(range(len(train_dataset) // 2)))) # half size\n",
    "train_dataloaders.append(DataLoader(big_train_dataset, batch_size=BATCH_SIZE, pin_memory=True)) # double size\n",
    "\n",
    "def train_offline(dataloader):\n",
    "    padim_offline = PADIM(\n",
    "        backbone=BACKBONE,\n",
    "        device=DEVICE,\n",
    "        backbone_features_idx=indices,\n",
    "        save_path=SAVE_PATH,\n",
    "        plot_metrics=True,\n",
    "    )\n",
    "    padim_offline.train(dataloader)\n",
    "\n",
    "def train_online(dataloader, number_of_features):\n",
    "    padim_online = PADIMWithOnlineCovariance(\n",
    "        backbone=BACKBONE,\n",
    "        device=DEVICE,\n",
    "        backbone_features_idx=indices,\n",
    "        save_path=SAVE_PATH,\n",
    "        plot_metrics=True,\n",
    "    )\n",
    "    padim_online.train(dataloader, number_of_features, int(CROP_SIZE/4), int(CROP_SIZE/4))\n",
    "\n",
    "def measure_memory(func, *args):\n",
    "    start_mem = memory_usage(max_usage=True)\n",
    "    return memory_usage((func, args), max_usage=True, retval=True)[0] - start_mem\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "# @profile\n",
    "# def train_offline():\n",
    "#     padim_offline.train(train_dataloader)\n",
    "\n",
    "# @profile\n",
    "# def train_online():\n",
    "#     padim_online.train(train_dataloader, NUMBER_OF_BACKBONE_FEATURES, int(CROP_SIZE/4), int(CROP_SIZE/4))\n",
    "    \n",
    " \n",
    "# start_mem = memory_usage(max_usage=True)\n",
    "# offline_usage = memory_usage(train_offline, max_usage=True, retval=True)[0] - start_mem\n",
    "# print(f'offline usage: {offline_usage}')\n",
    "\n",
    "# start_mem = memory_usage(max_usage=True)\n",
    "# online_usage = memory_usage(train_online, max_usage=True, retval=True)[0] - start_mem\n",
    "# print(f'online usage: {online_usage}')\n",
    "\n",
    "different_batches_mem = {'offline': [], 'online': []}\n",
    "for dataloader in train_dataloaders:\n",
    "    # different_batches_mem['offline'].append(measure_memory(train_offline, dataloader))\n",
    "    # different_batches_mem['online'].append(measure_memory(train_online, dataloader, NUMBER_OF_BACKBONE_FEATURES))\n",
    "    # print(different_batches_mem['offline'])\n",
    "    # print(different_batches_mem['online'])\n",
    "    %memit train_offline(dataloader)\n",
    "    %memit train_online(dataloader, NUMBER_OF_BACKBONE_FEATURES)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# different_backbone_size = {'offline': [], 'online': []}\n",
    "# backbones = [10, 30, 50, 100]\n",
    "# for number_of_backbones_features in backbones:\n",
    "#     different_backbone_size['offline'].append(measure_memory(train_offline))\n",
    "#     different_backbone_size['online'].append(measure_memory(train_online))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offline': [3414.66015625, 1555.2890625, 6733.6953125], 'online': [0.01171875, 0.05859375, 2.0234375]}\n"
     ]
    }
   ],
   "source": [
    "print(different_batches_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Your conclusions go here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Bonus task (optional)\n",
    "You can also add similar experiments with conclusions with regard to the `time` complexity. This task is optional, but if you'll loose points elsewhere, this can help you to make up for some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
